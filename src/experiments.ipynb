{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiments.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"18uDYEcOz4RGYEy4L_FK-76399Wyh_Fbw","authorship_tag":"ABX9TyMCL4r4IjfJeLV8o6h+1ZsK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QWnFw3kDV0R4","colab_type":"text"},"source":["## Dependencies, embeddings, transformers\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q3DM3oRYt8t0","colab":{}},"source":["! git clone https://github.com/josipjukic/Adversarial-NLP.git --quiet\n","% cd /content/Adversarial-NLP/src\n","\n","% mkdir .vector_cache\n","% cp '/content/drive/My Drive/Master Thesis/glove/glove.6B.100d.txt.pt' .vector_cache/\n","\n","! pip install transformers --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKEZU9wG1wuS","colab_type":"text"},"source":["## Experiments"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60-ztd4ch4Ws","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext import data\n","from torchtext import datasets\n","import spacy\n","from data_utils import (load_data, set_seed_everywhere)\n","from training import run_experiment\n","from metrics import init_tqdms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joKNAe6fWWbh","colab_type":"code","colab":{}},"source":["SEED = 42\n","set_seed_everywhere(SEED)\n","\n","LOAD_PATH = '/content/drive/My Drive/Master Thesis/AG'\n","MAX_VOCAB_SIZE = 25_000\n","EMBEDDINGS_FILE = 'glove.6B.100d'\n","\n","splits, fields = load_data(LOAD_PATH,\n","                           MAX_VOCAB_SIZE=MAX_VOCAB_SIZE,\n","                           EMBEDDINGS_FILE=EMBEDDINGS_FILE,\n","                           float_label=False)\n","TEXT, LABEL, *_ = fields"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMDRwRWl1UIl","colab_type":"code","colab":{}},"source":["from argparse import Namespace\n","from data_utils import expand_paths\n","from embeddings import get_embeddings\n","from models import PackedRNN\n","\n","dataset_name = 'AG'\n","rnn_type = 'LSTM'\n","bidirectional = True\n","identifier = ('bi' if bidirectional else '') + rnn_type\n","args = Namespace(\n","    # Data and Path hyper parameters\n","    model_save_file=f'{identifier}.torch',\n","    train_state_file=f'stats_{identifier}.json',\n","    save_dir=f'/content/drive/My Drive/Master Thesis/torch_models/{dataset_name}',\n","    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token],\n","    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token],\n","    # Model hyper parameters\n","    input_dim = len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_dim=256,\n","    output_dim = len(LABEL.vocab) if len(LABEL.vocab) > 2 else 1,\n","    num_layers=2,\n","    bidirectional=bidirectional,\n","    rnn_type=rnn_type,\n","    # Training hyper parameter\n","    seed=SEED,\n","    learning_rate=0.001,\n","    dropout_p=0.5,\n","    batch_size=64,\n","    num_epochs=20,\n","    early_stopping_criteria=5,\n","    # Runtime option\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",")\n","\n","expand_paths(args)\n","\n","model = PackedRNN(\n","    args.embedding_dim, \n","    args.hidden_dim, \n","    args.output_dim, \n","    args.num_layers,\n","    get_embeddings(TEXT, args),\n","    args.bidirectional,\n","    args.dropout_p, \n","    args.PAD_IDX,\n","    args.rnn_type,\n","    args.device\n",")\n","\n","model = model.to(args.device)\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    splits, \n","    batch_size=args.batch_size,\n","    sort_within_batch=True,\n","    sort_key = lambda x: len(x.text),\n","    device=args.device)\n","iterator = dict(train=train_iterator, valid=valid_iterator, test=test_iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82XKvjYviV6B","colab_type":"code","colab":{}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n","run_experiment(args, model, iterator, optimizer, criterion, init_tqdms(args, iterator))"],"execution_count":null,"outputs":[]}]}