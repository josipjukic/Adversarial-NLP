{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiments.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"18uDYEcOz4RGYEy4L_FK-76399Wyh_Fbw","authorship_tag":"ABX9TyPw8ayBD25l9unXLO/5ix6e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E0uOjyvR1dGw","colab_type":"text"},"source":["## Solving dependencies"]},{"cell_type":"markdown","metadata":{"id":"QWnFw3kDV0R4","colab_type":"text"},"source":["### Git repo, embedding, NLTK"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q3DM3oRYt8t0","colab":{}},"source":["! git clone https://github.com/josipjukic/Adversarial-NLP.git\n","% cd /content/Adversarial-NLP/src\n","\n","% mkdir .vector_cache\n","% cp '/content/drive/My Drive/Master Thesis/glove/glove.6B.100d.txt.pt' .vector_cache/\n","% cp '/content/drive/My Drive/Master Thesis/glove/counter-fitted-vectors.txt' .vector_cache/\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKEZU9wG1wuS","colab_type":"text"},"source":["## IMDb experiments"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60-ztd4ch4Ws","colab":{}},"source":["import torch\n","from torchtext import data\n","from torchtext import datasets\n","import spacy\n","from data_utils import load_dataset\n","from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"joKNAe6fWWbh","colab_type":"code","colab":{}},"source":["SEED = 42\n","torch.manual_seed(SEED)\n","LOAD_PATH = '/content/drive/My Drive/Master Thesis/IMDB'\n","MAX_VOCAB_SIZE = 25_000\n","EMBEDDINGS_FILE = 'glove.6B.100d'\n","\n","splits, fields = load_dataset(LOAD_PATH,\n","                              include_lengths=True,\n","                              lower=False,\n","                              stop_words=None,\n","                              load_raw=False,\n","                              load_id=False)\n","train_data, valid_data, test_data = splits\n","TEXT, LABEL, _, _ = fields\n","LABEL.build_vocab(train_data)\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = EMBEDDINGS_FILE, \n","                 unk_init = torch.Tensor.normal_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMDRwRWl1UIl","colab_type":"code","colab":{}},"source":["from argparse import Namespace\n","from data_utils import expand_paths\n","from models import PackedLSTM\n","\n","args = Namespace(\n","    # Data and Path hyper parameters\n","    model_save_file='imdb_model3.torch',\n","    train_state_file='train_state.json',\n","    save_dir='/content/drive/My Drive/Master Thesis/torch_models/imdb/',\n","    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token],\n","    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token],\n","    # Model hyper parameters\n","    input_dim = len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_dim=256,\n","    output_dim = 1,\n","    num_layers=2,\n","    bidirectional=True,\n","    # Training hyper parameter\n","    seed=SEED,\n","    learning_rate=0.001,\n","    dropout_p=0.5,\n","    batch_size=64,\n","    num_epochs=20,\n","    early_stopping_criteria=5,\n","    # Runtime option\n","    reload_from_files=True,\n","    expand_filepaths_to_save_dir=True,\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",")\n","\n","expand_paths(args)\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size=args.batch_size,\n","    sort_within_batch=True,\n","    sort_key = lambda x: len(x.text),\n","    device=args.device)\n","iterator = dict(train=train_iterator, valid=valid_iterator, test=test_iterator)\n","\n","pretrained_embeddings = TEXT.vocab.vectors\n","pretrained_embeddings[args.UNK_IDX] = torch.zeros(args.embedding_dim)\n","pretrained_embeddings[args.PAD_IDX] = torch.zeros(args.embedding_dim)\n","\n","model = PackedLSTM(\n","    args.embedding_dim, \n","    args.hidden_dim, \n","    args.output_dim, \n","    args.num_layers,\n","    pretrained_embeddings,\n","    args.bidirectional,\n","    args.dropout_p, \n","    args.PAD_IDX,\n","    args.device\n",")\n","\n","model = model.to(args.device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHW06qyRIC3H","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.optim as optim\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcalQG94rmw_","colab_type":"code","colab":{}},"source":["from tqdm.notebook import tqdm\n","\n","epoch_bar = tqdm(desc='Training routine', \n","                 total=args.num_epochs,\n","                 position=0)\n","\n","train_bar = tqdm(desc='Train set',\n","                 total=len(train_iterator), \n","                 position=1)\n","\n","val_bar = tqdm(desc='Valid set',\n","               total=len(valid_iterator), \n","               position=1)\n","\n","tqdms = dict(main=epoch_bar, train=train_bar, valid=val_bar)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82XKvjYviV6B","colab_type":"code","colab":{}},"source":["from training import run_experiment\n","\n","run_experiment(args, model, iterator, optimizer, criterion, tqdms)"],"execution_count":0,"outputs":[]}]}