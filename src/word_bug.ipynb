{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_bug.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1o7HFeQZ2SnaruHric3ena83uRp22_pjh","authorship_tag":"ABX9TyPTpxnfDX8Rs/6BnxFOC07y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"E0uOjyvR1dGw","colab_type":"text"},"source":["## Solving dependencies"]},{"cell_type":"markdown","metadata":{"id":"QWnFw3kDV0R4","colab_type":"text"},"source":["### Git repo, embedding, NLTK"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q3DM3oRYt8t0","colab":{}},"source":["! git clone https://github.com/josipjukic/Adversarial-NLP.git\n","% cd /content/Adversarial-NLP/src\n","\n","% mkdir .vector_cache\n","% cp '/content/drive/My Drive/Master Thesis/glove/glove.6B.100d.txt.pt' .vector_cache/\n","% cp '/content/drive/My Drive/Master Thesis/glove/counter-fitted-vectors.txt' .vector_cache/\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKEZU9wG1wuS","colab_type":"text"},"source":["## Dataset save/load"]},{"cell_type":"code","metadata":{"id":"fAG4WVtC1LUe","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data\n","from torchtext import datasets\n","import spacy\n","import random\n","from preprocessing import imdb_preprocess\n","from data_utils import load_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"joKNAe6fWWbh","colab_type":"code","colab":{}},"source":["SEED = 42\n","torch.manual_seed(SEED)\n","LOAD_PATH = '/content/drive/My Drive/Master Thesis/IMDB'\n","MAX_VOCAB_SIZE = 25_000\n","EMBEDDINGS_FILE = 'glove.6B.100d'\n","\n","splits, fields = load_dataset(LOAD_PATH,\n","                              include_lengths=True,\n","                              lower=False,\n","                              stop_words=None)\n","train_data, valid_data, test_data = splits\n","TEXT, LABEL, RAW, ID = fields\n","RAW.is_target = ID.is_target = False\n","LABEL.build_vocab(train_data)\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = EMBEDDINGS_FILE, \n","                 unk_init = torch.Tensor.normal_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSSsflavpNZD","colab_type":"code","colab":{}},"source":["from argparse import Namespace\n","from data_utils import expand_paths\n","from models import PackedLSTM\n","\n","args = Namespace(\n","    # Data and Path hyper parameters\n","    model_path='/content/drive/My Drive/Master Thesis/torch_models/imdb/imdb_model.torch',\n","    train_state_file='train_state.json',\n","    save_dir='/content/drive/My Drive/Master Thesis/torch_models/imdb/',\n","    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token],\n","    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token],\n","    # Model hyper parameters\n","    input_dim = len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_dim=256,\n","    output_dim = 1,\n","    num_layers=2,\n","    bidirectional=True,\n","    # Training hyper parameter\n","    seed=SEED,\n","    learning_rate=0.001,\n","    dropout_p=0.5,\n","    batch_size=64,\n","    num_epochs=20,\n","    early_stopping_criteria=5,\n","    # Runtime option\n","    reload_from_files=True,\n","    expand_filepaths_to_save_dir=True,\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",")\n","\n","pretrained_embeddings = TEXT.vocab.vectors\n","pretrained_embeddings[args.UNK_IDX] = torch.zeros(args.embedding_dim)\n","pretrained_embeddings[args.PAD_IDX] = torch.zeros(args.embedding_dim)\n","\n","model = PackedLSTM(\n","    args.embedding_dim, \n","    args.hidden_dim, \n","    args.output_dim, \n","    args.num_layers,\n","    pretrained_embeddings,\n","    args.bidirectional,\n","    args.dropout_p, \n","    args.PAD_IDX,\n","    args.device\n",")\n","model.load_state_dict(torch.load(args.model_path, map_location=args.device))\n","model = model.to(args.device)\n","model.eval()\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size=512,\n","    sort_within_batch=True,\n","    sort_key = lambda x: len(x.text),\n","    device=args.device)\n","iterator = dict(train=train_iterator, valid=valid_iterator, test=test_iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVfNJGOvCbxL","colab_type":"code","colab":{}},"source":["def word_target(model, batch, y_preds, num_classes, device):\n","    inputs = batch[0]\n","    losses = torch.zeros(inputs.shape)\n","    target = None\n","    for i in range(inputs.shape[0]):\n","        if target:\n","            index, vals = target\n","            inputs[i-1,:] = vals\n","        target = (i, torch.clone(inputs[i,:]))\n","        inputs[i,:] = 0\n","        with torch.no_grad():\n","            out = model.predict_proba(batch)\n","            if num_classes == 2:\n","                out = torch.cat([1.-out, out], dim=1)\n","            losses[i,:] = out.gather(1, y_preds).squeeze()\n","    \n","    if target:\n","        index, vals = target\n","        inputs[-1,:] = vals\n","    return 1.-losses\n","\n","\n","def temporal(model, batch, y_preds, num_classes, device):\n","    inputs, lengths = batch\n","    new_preds = torch.zeros(inputs.shape)\n","    losses = torch.zeros(inputs.shape)\n","    for i in range(inputs.shape[0]):\n","        preinputs = inputs[:i+1,:]\n","        with torch.no_grad():\n","            new_lengths = torch.min(lengths, torch.tensor(i+1).to(device))\n","            preout = model.predict_proba((preinputs, new_lengths))\n","            if num_classes == 2:\n","                preout = torch.cat([1.-preout, preout], dim=1).to(device)\n","            new_preds[i,:] = preout.gather(1, y_preds).squeeze()\n","            \n","    losses[0,:] = new_preds[0,:] - 1.0/num_classes\n","    for i in range(1, inputs.shape[0]):\n","        losses[i,:] = new_preds[i,:] - new_preds[i-1,:]\n","\n","    return losses\n","\n","\n","def temporal_tail(model, batch, y_preds, num_classes, device):\n","    inputs, lengths = batch\n","    new_preds = torch.zeros(inputs.shape)\n","    losses = torch.zeros(inputs.shape)\n","    for i in range(inputs.shape[0]):\n","        postinputs = inputs[i:,:]\n","        with torch.no_grad():\n","            new_lengths = torch.max(lengths-i, torch.tensor(1).to(device))\n","            postout = model.predict_proba((postinputs, new_lengths))\n","            if num_classes == 2:\n","                postout = torch.cat([1.-postout, postout], dim=1).to(device)\n","            new_preds[i,:] = postout.gather(1, y_preds).squeeze()\n","            \n","    losses[-1,:] = new_preds[-1,:] - 1.0/num_classes\n","    for i in range(inputs.shape[0]-1):\n","        losses[i,:] = new_preds[i,:] - new_preds[i+1,:]\n","\n","    return losses\n","\n","\n","def combined_temporal(model, batch, y_preds, num_classes, device, alpha=1.):\n","    temporal_score = temporal(model, batch, y_preds, num_classes, device)\n","    temporal_tail_score = temporal_tail(model, batch, y_preds, num_classes, device)\n","    return temporal_score + alpha*temporal_tail_score\n","\n","\n","def random(inputs, *args, **kwargs):\n","    losses = torch.rand(inputs.size()[0], inputs.size()[1])\n","    return losses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLKthJhJmjxG","colab_type":"code","colab":{}},"source":["from data_utils import spacy_revtok\n","import numpy as np\n","\n","num_classes = len(LABEL.vocab)\n","nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n","\n","def reconstruct(tensor, vocab):\n","    words = [vocab.itos[idx] for idx in tensor]\n","    return ' '.join(words)\n","\n","def adversarial_text(raw, nlp, indices, transform):\n","    adv_words = [token.text_with_ws for token in nlp(raw)]\n","    for i in indices:\n","        if i >= len(adv_words): continue\n","        adv_words[i] = transform(adv_words[i])\n","    return ''.join(adv_words)\n","\n","def binary_accuracy(model, batch):\n","    y_pred = model.predict(batch.text).to(args.device).squeeze()\n","    correct = (y_pred == batch.label).float()\n","    return (correct.sum() / len(correct)).item()\n","\n","homos = {'-':'Àó','9':'‡ß≠','8':'»¢','7':'ùüï','6':'–±','5':'∆º','4':'·èé','3':'∆∑','2':'·íø','1':'l','0':'O',\n","         \"'\":'`','a': '…ë', 'b': '–¨', 'c': 'œ≤', 'd': '‘Å', 'e': '–µ', 'f': 'ùöè', 'g': '…°', 'h': '’∞',\n","         'i': '—ñ', 'j': 'œ≥', 'k': 'ùíå', 'l': '‚Öº', 'm': 'ÔΩç', 'n': '’∏', 'o':'–æ', 'p': '—Ä', 'q': '‘õ',\n","         'r': '‚≤Ö', 's': '—ï', 't': 'ùöù', 'u': '’Ω', 'v': '—µ', 'w': '‘ù', 'x': '√ó', 'y': '—É', 'z': '·¥¢'}\n","\n","def homoglyph(word):\n","    N = len(word)-1 if word[-1] == ' ' else len(word)\n","    N = max(1, N)\n","    s = np.random.randint(0, N)\n","    if word[s] in homos: \n","        adv_char = homos[word[s]]\n","    else:\n","        adv_char = word[s]\n","    adv_word = word[:s] + adv_char + word[s+1:]\n","    return adv_word\n","\n","def remove_char(word):\n","    N = len(word)-1 if word[-1] == ' ' else len(word)\n","    N = max(1, N)\n","    s = np.random.randint(0, N)\n","    adv_word = word[:s] + word[s+1:]\n","    return adv_word\n","\n","def flip_char(word):\n","    N = len(word)-1 if word[-1] == ' ' else len(word)\n","    N = max(1, N)\n","    s = np.random.randint(0, N)\n","    letter = ord(word[s])\n","    adv_char = np.random.randint(0,25) + 97\n","    adv_word = word[:s] + chr(adv_char) + word[s+1:]\n","    return adv_word\n","\n","attack_power = 20\n","reg_acc = 0.\n","adv_acc = 0.\n","for batch_index, batch in enumerate(iterator['test'], 1):\n","    # print('Length: ', batch.text[0].shape[0])\n","    print('Batch: ', batch_index)\n","    x_in, lengths = batch.text\n","    y_preds = model.predict(batch.text).to(args.device)\n","    losses = word_target(model=model, batch=batch.text,\n","                         y_preds=y_preds, num_classes=num_classes,\n","                         device=args.device)\n","    sorted_losses, indices = torch.sort(losses, dim=0, descending=True)\n","    acc_t = binary_accuracy(model, batch)\n","    reg_acc += (acc_t - reg_acc) / batch_index\n","\n","\n","    for i in range(x_in.shape[1]):\n","        inds = indices[0:attack_power,i]\n","        x_in[inds,i] = 0\n","        # print(adversarial_text(batch.raw[i], nlp, inds, flip_char))\n","        # print(batch.raw[i])\n","\n","    acc_t = binary_accuracy(model, batch)\n","    adv_acc += (acc_t - adv_acc) / batch_index\n","    print(reg_acc, adv_acc)\n","    print('-----------------------------')\n","\n","    \n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ih9PCI-C7NNO","colab_type":"code","colab":{}},"source":["from data_utils import save_dataset\n","\n","SAVE_PATH = '/content/drive/My Drive/Master Thesis/IMDB'\n","\n","dataset = dict(train=train_data, test=test_data, valid=valid_data)\n","save_dataset(dataset, SAVE_PATH)"],"execution_count":0,"outputs":[]}]}