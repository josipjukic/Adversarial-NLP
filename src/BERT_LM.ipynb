{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_LM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1FJo_Ka86eIEHbUOguOyiH-LMmGVVce_Y","authorship_tag":"ABX9TyNcfTV3UxziKJCXqh6Reyx2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E0uOjyvR1dGw","colab_type":"text"},"source":["## Solving dependencies\n"]},{"cell_type":"markdown","metadata":{"id":"QWnFw3kDV0R4","colab_type":"text"},"source":["### Git repository, embeddings, NLTK"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q3DM3oRYt8t0","colab":{}},"source":["! git clone https://github.com/josipjukic/Adversarial-NLP.git\n","% cd /content/Adversarial-NLP/src\n","\n","% mkdir .vector_cache\n","% cp '/content/drive/My Drive/Master Thesis/glove/glove.6B.100d.txt.pt' .vector_cache/\n","% cp '/content/drive/My Drive/Master Thesis/glove/counter-fitted-vectors.txt' .vector_cache/\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKEZU9wG1wuS","colab_type":"text"},"source":["## IMDb experiments"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60-ztd4ch4Ws","colab":{}},"source":["import torch\n","from torchtext import data\n","from torchtext import datasets\n","import spacy\n","from data_utils import load_dataset\n","from nltk.corpus import stopwords"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"joKNAe6fWWbh","colab_type":"code","colab":{}},"source":["SEED = 42\n","torch.manual_seed(SEED)\n","LOAD_PATH = '/content/drive/My Drive/Master Thesis/IMDB'\n","MAX_VOCAB_SIZE = 25_000\n","EMBEDDINGS_FILE = 'glove.6B.100d'\n","\n","splits, fields = load_dataset(LOAD_PATH,\n","                              include_lengths=True,\n","                              lower=False,\n","                              stop_words=None,\n","                              load_raw=False,\n","                              load_id=False)\n","train_data, valid_data, test_data = splits\n","TEXT, LABEL, _, _ = fields\n","LABEL.build_vocab(train_data)\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = EMBEDDINGS_FILE, \n","                 unk_init = torch.Tensor.normal_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VL_9R1av7z7M","colab_type":"code","colab":{}},"source":["from transformers import BertTokenizer, BertForMaskedLM\n","import torch\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased', ).to('cuda')\n","\n","tokens = tokenizer.encode(\"hello, my [MASK] is cute\")\n","input_ids = torch.tensor(tokens).unsqueeze(0).to('cuda')  # Batch size 1\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","\n","loss, prediction_scores = outputs[:2]\n","prediction_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hzHNxkvug6K","colab_type":"code","colab":{}},"source":["tks = tokenizer.encode(\"cat dog weekend\", add_special_tokens=False)\n","prediction_scores[0,7, tks]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8S9rNN3Q8CTO","colab_type":"code","colab":{}},"source":["# ! pip install transformers\n","from transformers import BertTokenizer, BertForMaskedLM\n","import numpy as np\n","import torch\n","\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","\n","class MaskedLM():\n","    def __init__(self, LM=None, tokenizer=None,\n","                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n","    \n","        self.LM = LM\n","        self.tokenizer = tokenizer\n","        self.device = device\n","        if self.LM is None:\n","            name = 'bert-base-uncased'\n","            self.tokenizer = BertTokenizer.from_pretrained(name)\n","            self.LM = BertForMaskedLM.from_pretrained(name).to('cuda')\n","            \n","\n","    def substitution_score(self, sentence, target_index, subs):\n","        target = sentence[target_index]\n","        x_in = torch.tensor([self.tokenizer.convert_tokens_to_ids(sentence)],\n","                            device=self.device)\n","        sentence[target_index] = '[MASK]'\n","        mask_in = torch.tensor([self.tokenizer.convert_tokens_to_ids(sentence)],\n","                               device=self.device)\n","        sentence[target_index] = target\n","\n","        sub_ids = self.tokenizer.convert_tokens_to_ids(subs)\n","        with torch.no_grad():\n","            preds = self.LM(mask_in, masked_lm_labels=x_in)[1][0,target_index]\n","\n","        indices = torch.argsort(preds[sub_ids], descending=True)\n","        ordered_subs = [subs[i] for i in indices]\n","        return ordered_subs\n","\n","    def _get_candidates(self, sentence, target_index, n_substitutes=10):\n","        target = sentence[target_index]\n","        x_in = torch.tensor([self.tokenizer.convert_tokens_to_ids(sentence)],\n","                            device=self.device)\n","        sentence[target_index] = '[MASK]'\n","        mask_in = torch.tensor([self.tokenizer.convert_tokens_to_ids(sentence)],\n","                               device=self.device)\n","        sentence[target_index] = word\n","\n","        with torch.no_grad():\n","            preds = self.LM(mask_in, masked_lm_labels=x_in)[1][0,target_index]\n","            _, top_k_index = torch.topk(preds, n_substitutes)\n","            return self.tokenizer.convert_ids_to_tokens(top_k_index.tolist())\n","\n","# lex_sub = LexSub()\n","a = 'You are the son of my dad.'\n","a = lex_sub.tokenizer.tokenize(a)\n","print(a)\n","subs, topk = lex_sub.substitution_score(a, 3, ['daughter', 'wife', 'sister', 'brick', 'shoe'])\n","\n","print(subs, topk)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAD2gyyuEDbx","colab_type":"code","colab":{}},"source":["\n","from collections import defaultdict\n","import numpy as np \n","from numpy.linalg import norm\n","from gensim.models import KeyedVectors\n","import string\n","import re\n","from abc import ABC, abstractmethod\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from nltk.corpus import lin_thesaurus\n","\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","class WordNetTagger():\n","    def __init__(self):\n","        self.tag_map = defaultdict(\n","            lambda: None,\n","            {'NN':wordnet.NOUN, 'JJ':wordnet.ADJ,\n","             'VB':wordnet.VERB, 'RB':wordnet.ADV}\n","        )\n","      \n","    def tag(self, sentence):\n","        self.tags = pos_tag(sentence)\n","\n","    def get_tag(self, index):\n","        return self.tag_map[self.tags[index][1][:2]]\n","\n","\n","\n","def load_vocab_embeddings(path, vocab, emb_dim=300):\n","    emb_mat = np.zeros((emb_dim, len(vocab)))\n","    with open(path, 'r') as f:\n","        for line in f:\n","            row = line.strip().split(' ')\n","            word = row[0]\n","            i = vocab.stoi[word]\n","            if i == 0: continue\n","            emb_mat[:,i] = np.array(row[1:]).astype(np.float)\n","        return emb_mat\n","\n","\n","class LexSubBase(ABC):\n","    def __init__(self, vocab):\n","        self.vocab = vocab\n","        stop_words = stopwords.words('english')\n","        punkt = string.punctuation\n","        self.spec_words = set()\n","        self._add_spec_words(stop_words)\n","        self._add_spec_words(punkt)\n","\n","    def _add_spec_words(self, words):\n","        for el in words:\n","            idx = self.vocab.stoi[el]\n","            if idx > 0:\n","                self.spec_words.add(idx)\n","\n","    @abstractmethod\n","    def get_candidates(self):\n","        pass\n","\n","    @abstractmethod\n","    def sort_by_substitutability(self):\n","       pass\n","    \n","\n","class LexSub(LexSubBase):\n","    \"Find word substitutions for a word in context using word2vec skip-gram embedding\"\n","    def __init__(self, vocab,\n","                 vector_path='.vector_cache/counter-fitted-vectors.txt'):\n","        super().__init__(vocab)\n","        self.emb_mat = load_vocab_embeddings(vector_path, vocab)\n","\n","        c_ = -2*np.dot(self.emb_mat.T, self.emb_mat)\n","        a = np.sum(np.square(self.emb_mat), axis=0).reshape((1,-1))\n","        b = a.T\n","        self.dist_mat = a+b+c_\n","\n","    def get_candidates(self, words, n_substitutes=10,\n","                       n_candidates=10, sentence=None):\n","        \n","        cand_list = []\n","        for i, word in enumerate(words):\n","            cands = self._get_candidates(\n","                        target=word,\n","                        target_index=i,\n","                        n_candidates=n_candidates,\n","                        sentence=sentence\n","                     )\n","            if sentence:\n","                cands = self.sort_by_substitutability(cands, word, i, words,\n","                                                      n_substitutes)\n","            else:\n","                cands = cands[:n_substitutes]\n","            cand_list.append(cands)\n","\n","        return cand_list\n","\n","    def sort_by_substitutability(self, cands, target, target_index,\n","                                 sentence, n_substitutes):\n","        C = [c for c in sentence if c not in self.spec_words and c != target]\n","        scores = [self.get_substitutability(target, target_index, cand, C)\n","                  for cand in cands]\n","        sorted_cands = sorted(zip(cands, scores), key = lambda x : x[1])\n","        return [sub for sub, _ in sorted_cands][:n_substitutes]\n","\n","    def get_substitutability(self, t, ti, s, C):\n","        \"\"\"\n","        t = target word\n","        ti = target index\n","        s = candidate substitution \n","        C = list of context words \n","        \"\"\"\n","        tscore = self.dist_mat[t][s]\n","        \n","        if len(C) == 0:\n","            cscore = 0\n","        else:\n","            cscores = [self.dist_mat[t][c] for c in C ]\n","            cscore = sum(cscores) / (len(C))\n","\n","        return tscore + cscore\n","\n","    @abstractmethod\n","    def _get_candidates(self, **kwargs):\n","        pass\n","\n","\n","class SynonymModel(LexSub):\n","    def _get_candidates(self, target, n_candidates=10, **kwargs):\n","        if target == 0: return []\n","        return np.argsort(self.dist_mat[target,:])[1:1+n_candidates]\n","\n","\n","class WordnetModel(LexSub):\n","    def __init__(self, vocab,\n","                 vector_path='.vector_cache/counter-fitted-vectors.txt'):\n","        super().__init__(vocab, vector_path)\n","        self.tagger = WordNetTagger()\n","\n","    def get_candidates(self, **kwargs):\n","        self.tagger.tag(kwargs['sentence'])\n","        return super().get_candidates(**kwargs)\n","    \n","    def _get_candidates(self, target, target_index, n_candidates, **kwargs):\n","        if target == 0: return []\n","        tag = self.tagger.get_tag(target_index)\n","        word = self.vocab.itos[target]\n","        syns = WordnetModel.wordnet_synonyms(word, tag)\n","        cands = set()\n","        for syn in syns:\n","            if syn != word:\n","                id = self.vocab.stoi[syn]\n","                if id != 0:\n","                    cands.add(id)\n","        return list(cands)[:n_candidates]\n","\n","    @staticmethod\n","    def wordnet_synonyms(word, pos_tag):\n","        synset = wordnet.synsets(word, pos_tag)\n","        return [lemma.name() for s in synset for lemma in s.lemmas()]\n","\n","\n","class LinModel(LexSub):\n","    def __init__(self, vocab,\n","                 vector_path='.vector_cache/counter-fitted-vectors.txt'):\n","        super().__init__(vocab, vector_path)\n","        self.tagger = WordNetTagger()\n","\n","    def get_candidates(self, **kwargs):\n","        self.tagger.tag(kwargs['sentence'])\n","        return super().get_candidates(**kwargs)\n","    \n","    def _get_candidates(self, target, target_index, n_candidates, **kwargs):\n","        if target == 0: return []\n","        tag = self.tagger.get_tag(target_index)\n","        word = self.vocab.itos[target]\n","        syns = LinModel.lin_synonyms(word, tag)\n","        cands = []\n","        for syn in syns:\n","            if syn != word:\n","                id = self.vocab.stoi[syn]\n","                if id != 0:\n","                    print(syn)\n","                    cands.append(id)\n","        return list(cands)[:n_candidates]\n","\n","    @staticmethod\n","    def lin_synonyms(word, pos):\n","        fileid = 'sim%s.lsp' % pos.upper()\n","        thes_entry = lin_thesaurus.scored_synonyms(word, fileid=fileid)\n","        thes_entry = sorted(thes_entry, key = (lambda x : x[1]), reverse = True)\n","        return [syn for syn, score in thes_entry]\n","\n","sm = WordnetModel(TEXT.vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbNzqxvo-iRG","colab_type":"code","colab":{}},"source":["sent = ['dog', 'is', 'sitting']\n","words = [TEXT.vocab.stoi[s] for s in sent]\n","a = sm.get_candidates(words=words, n_substitutes=10, n_candidates=10, sentence=sent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PeYkAuoFARMS","colab_type":"code","colab":{}},"source":["print(a[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSSvD2-zEF51","colab_type":"code","colab":{}},"source":["from nltk import pos_tag\n","import nltk\n","from nltk.corpus import wordnet\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from collections import defaultdict\n","\n","class WordNetTagger():\n","    def __init__(self):\n","        self.tag_map = defaultdict(\n","            lambda: None,\n","            {'NN':wordnet.NOUN, 'JJ':wordnet.ADJ,\n","             'VB':wordnet.VERB, 'RB':wordnet.ADV}\n","        )\n","      \n","    def tag(self, sentence):\n","        self.tags = pos_tag(sentence)\n","\n","    def get_tag(self, index):\n","        return self.tag_map[self.tags[index][1][:2]]\n","\n","s = WordNetTagger()\n","s.tag(['movie', 'my', 'dear', 'friend', '!', '?', 'mouse', 'is', 'rolling'])\n","print(s.get_tag(3))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZ8Sg2JmK72n","colab_type":"code","colab":{}},"source":["import nltk\n","# nltk.download('wordnet')\n","from nltk.corpus import wordnet\n","\n","synonyms = []\n","antonyms = []\n","\n","for syn in wordnet.synsets('animal') :\n","    for l in syn.lemmas():\n","        print(l.hypernyms())\n","        synonyms.append(l.name())\n","        if l.antonyms():\n","                antonyms.append(l.antonyms()[0].name())\n","\n","print(set(synonyms))\n","print(set(antonyms))\n","print(wordnet.ADJ_SAT)"],"execution_count":0,"outputs":[]}]}